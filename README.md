# RUAuto_CV_training

Required: Python, Tensorflow, Pillow (PIL), Numpy, and MatplotLib

### DeepDream:
  
  DeepDream is a way to visualize the weights of deep convolutional neural networks using a technique called "Optimization". Viewing the raw weights 
  can be difficult for humans to interpret, except the first few layers of the network which form edge detectors. DeepDream works by choosing a layer
  in the network and changing the raw pixels of the input image (via Gradient Descent) such that some function of the chosen layer's output 
  is maximized. The DeepDream algorithm is a little more complex than this, but this is the most important part. If you are interested
  in seeing other ways you can visualize network weights, look into "Deconvolution" also called by its proper term "Tranposed Convolution."
  You will find that the DeepDream results are a lot better.
  
  Also here is the online DeepDream generator : https://deepdreamgenerator.com/
  And DeepArt which is a little more complex (hopefully we will get to it in the training sessions) : https://deepart.io/
  
  To use deep_dream.py -> 
    run get_ckpy.py : python get_ckpy.py
    then run : python deep_dream.py --image_path {insert path to image you want to deep dream}
  
### GAN (Generative Adversarial Network):

  A GAN is not a traditional Neural Network, it is actually a probabilistic model, unlike normal neural networks which model funtions.
  A GAN is a specific class of Differentiable Generative Networks (a probability distribution modeled by a differentiable function).
  Thus GAN's model PROBABILITY distributions and thus we can sample from them.
  A GAN consists of two Networks, a Discriminator and Generator:
    The Generator takes as input a "code", which is a N dimenstional vector of Real numbers, and outputs a sample from the distribution
    (i.e. an image)
    The Discriminator learns to differentiate between samples generated by the Generator and samples from the training set.
  This trains the Generator to generate samples that look like they came from the training set.
  They eventually reach an equallibrium and we can then drop the Discriminator. Now by inputing random vectors to the Generator
  we can generate new samples from the probability distribution that were not in the training set (i.e. crazy new images).
  
  Another type of Differentiable Generative Net is called a Variational Autoencoder(VAE) (see my github). You will find the GAN produces higher quality
  images.
  
  To use it: python gan.py
  
### Autoencoder :

  An Autoencoder is a type of Neural Network that learns to reproduce its input at its output.  While this sounds pretty useless,
  it can be modified to do really cool things such as : generate Hashes (i.e. for Databases), generate images (VAE), reconstruct
  noisy input, and compression. An Autoencoder essentially compresses its input to a lower dimension (from layers with lots of 
  units to layers with few units), and then increases the dimension until it produces the input at it's output (goes from layers with
  few units to layers with lots). The program in this repo implements a traditional Autoencoder and a noisy Autoencoder (for noisy input reconstruction). 
  However, it has some issues which we might discuss at future training sesssions. It can be modifed to learn more usual featues about the dataset.
  
  To use it: python autoencoder.py
  
